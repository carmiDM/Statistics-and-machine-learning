{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf12c8e-d842-443a-ba9c-621f01183d98",
   "metadata": {},
   "source": [
    "# 4.17.41 Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c5778-ae59-4ba8-a473-d417dc9c9e2c",
   "metadata": {},
   "source": [
    "### Design of Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ed20b-931e-4485-b1d6-2b69e69de7ca",
   "metadata": {},
   "source": [
    "The idea of **[design of experiments](https://en.wikipedia.org/wiki/Design_of_experiments)** has its origins in traditional statistics and it has applications in many areas of research. The idea is to appropriately **design an experiment in order to confirm or reject a hypothesis**. \n",
    "\n",
    "Data analysts and data scientists are often faced with the need to conduct experiments regarding user interfaces, product marketing, pricing and so on. Something to keep in mind is that statistical testing should always be used to confirm or reject a hypothesis, not to find confirmation in one's ideas or gut feel; as the scottish poet Andrew Lang wisely said: \n",
    "\n",
    "> He uses statistics as a drunken man uses lamp-posts... for support rather than illumination. \n",
    "\n",
    "This is roughly how the process works: \n",
    "\n",
    "1. you formulate a hypothesis, such as “drug A is better than the existing standard drug” or, in a business context, “this new landing page generates more conversions than the existing one”;\n",
    "2. an experiment (such as an **A/B test**) is designed specifically to test the hypothesis; \n",
    "3. you start collecting the relevant data and analyze it; \n",
    "4. a conclusion is inferred based on the results. \n",
    "\n",
    "<img src=\"img/inference-pipeline.png\" width=\"900\">\n",
    "\n",
    "The term **[inference](https://en.wikipedia.org/wiki/Statistical_inference)** reflects the fact that we want to generalise the experiment results, which involve a limited set of data (like a randomly chosen subset of customers), to a larger population (all the customers). \n",
    "\n",
    "A well conducted experiment should be well planned and documented, the objective metric should be chosen carefully and in case of an **[AB test](https://en.wikipedia.org/wiki/A/B_testing)**, particular care should be taken in the randomisation of test and control groups as well as the aving reached a large enough sample size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e8ca0-0194-4ac4-abe3-d0366c6d4921",
   "metadata": {},
   "source": [
    "### Tests of Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937283a-dc2b-4b17-9528-e73095e397b7",
   "metadata": {},
   "source": [
    "The definition on Wikipedia says that: \n",
    "\n",
    "> A **[statistical hypothesis test](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing)** is a method of statistical inference used to decide whether the data at hand sufficiently **support a particular hypothesis**. Hypothesis testing allows us to make **probabilistic statements about population** parameters.\n",
    "\n",
    "It should be clear from the previous chapter that we should test something only after we have duly formulated a hypothesis and designed the process that defines the experiment so that the test's conclusions can be considered valid and reliable. \n",
    "\n",
    "At the last step of our design of experiment process, after we have collected the data, it's time to **test whether our hypothesis is validated by the data** or not. It turns out that there are **different tests** that can be conducted depending on the type of phenomena we are measuring; the following flowchart can help us distinguish all the various cases: \n",
    "\n",
    "<img src=\"img/hyp-test-diagram.png\" width=\"800\">\n",
    "\n",
    "In most business cases you will have enough data and will need to use either a z-test or a t-test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6b4b4-4b84-45db-b053-499fbedd5dc7",
   "metadata": {},
   "source": [
    "#### z-test\n",
    "\n",
    "A **[z-test](https://en.wikipedia.org/wiki/Z-test)** is a statistical test for which the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution. It is helpful to determine whether two population means are different when the variances are known and the sample size is large. \n",
    "\n",
    "Leaving most of the [technicalities](https://www.omnicalculator.com/statistics/z-test) aside, you can [perform a z-test in Python](https://www.statology.org/z-test-python/) using `ztest()` function from the `statsmodels` library (in the `stats.weightstats` module). \n",
    "\n",
    "z-tests are closely related to t-tests, but t-tests are best performed when an experiment has a small sample size and qhen the standard deviation is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e29ebc-38f3-4d5a-9b5e-c68016860bef",
   "metadata": {},
   "source": [
    "#### t-test\n",
    "\n",
    "A **[t-test](https://en.wikipedia.org/wiki/Student%27s_t-test)** can be used to to determine if the means of two sets of data are significantly different from each other. In a t-test, the test statistic follows a Student's t-distribution under the null hypothesis. \n",
    "\n",
    "Shaped like a Normal distribution, the **[t-distribution](https://en.wikipedia.org/wiki/Student's_t-distribution)**'s tails are a bit thicker and longer, meaning that it is more prone to producing values that fall far from its mean, which explains why it is better suited for cases in which the variance is unknown. The larger the sample, the more normally shaped the t-distribution becomes.\n",
    "\n",
    "<img src=\"img/t-distr-vs-z-distr.png\" width=\"400\">\n",
    "\n",
    "As we just saw, it is widely used in describing distributions of sample statistics, including **Student's t-test**. Python's `scipy` library offers the `scipy.stats.ttest_ind()` function that can be used to [calculate the t-test](https://analyticsindiamag.com/a-beginners-guide-to-students-t-test-in-python-from-scratch%EF%BF%BC/) for the means of two independent samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfebfea-de21-4a28-be60-16f825c3f9bc",
   "metadata": {},
   "source": [
    "#### Binomial test\n",
    "\n",
    "Out of completeness, it is fair to mention the **[binomial test](https://en.wikipedia.org/wiki/Binomial_test)** as well, which compares a sample proportion to a hypothesized proportion and thus can be used to test hypotheses about the probability of success for cases with small sample sizes. \n",
    "\n",
    "It can be [easily computed](https://www.statology.org/binomial-test-python/) using the `scipy.stats.binom_test()` function from the `scipy` library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71055ddc-3980-4f63-8f48-b45489e15c68",
   "metadata": {},
   "source": [
    "#### Permutation tests\n",
    "\n",
    "*Note: permutation tests are a non-parametric alternative to the parametric tests listed above.*\n",
    "\n",
    "All the tests we saw above are somehow **tied to assumptions** around the underlying probability distribution that generated the process that we are observing. They are, in other words, **parametric tests**, because the assumption is made that the samples are normally distributed. \n",
    "\n",
    "**[Permutation tests](https://en.wikipedia.org/wiki/Permutation_test)** are non-parametric tests that only required that the sequence of data is [exchangeable](https://en.wikipedia.org/wiki/Exchangeable_random_variables#Examples). \n",
    "\n",
    "> A sequence is exchangeable if changing the order of the sequence (permutation) does not change the joint probability distribution of the original data: a sequence of fair coin tosses is exchangeable, while most time series are not exchangeable as previous values can determine future values.  \n",
    "\n",
    "The **main difference** between parametric and non-parametric tests is that the former leverage assumptions to create the distribution whereas the latter leverage resampling. \n",
    "\n",
    "<img src=\"img/param-non-param.png\" width=\"400\">\n",
    "\n",
    "**[Resampling](https://en.wikipedia.org/wiki/Resampling_(statistics))** is the practice of repeatedly sample values from your own data; there are two main types of resampling: \n",
    "\n",
    "- **bootstrapping**: is used to assess the reliability of an estimate, which we discussed in our last class;\n",
    "- **permutation tests**: are used to test hypotheses, usually involving two or more groups.\n",
    "\n",
    "In a permutation test, two or more samples are involved, typically the groups in an A/B or other hypothesis test. Like in any other hypothesis test, the goal is to test an hypothesis, but instesad of relying on formulas and distribution assumptions, they leverage the power of resampling. \n",
    "\n",
    "There are many great online resources on hypothesis testing and permutation tests, but the following website by [Jared Wilber](https://www.jwilber.me/) has the best visual explanation of its workings, so I decided to use it as a prop for this part of the class (also, check out the next cell to see how to embed a website in a Jupyter Notebook's cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9aea856-2f49-4469-b722-09f6e05a992d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"700px\"\n",
       "            src=\"https://www.jwilber.me/permutationtest/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f915e558ac0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src=\"https://www.jwilber.me/permutationtest/\", width='100%', height='700px')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c28ec6-8986-4b8b-86be-02a337eb45cf",
   "metadata": {},
   "source": [
    "Let's try to replicate the example we just saw in Jared's great post using Python. Of course, the randomisation procedure and the response value assignment are part of the design and execution of the experiment, so we can skip them for now, since our main objective here is to test our hypothesis and reach a conclusion. \n",
    "\n",
    "As stated by the article above, the following are the null and alternative hypothesis: \n",
    "- $H_0: \\mu_{treatment} = \\mu_{control}$\n",
    "- $H_1: \\mu_{treatment} > \\mu_{control}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0b3cad-2ab9-4448-9d9e-c9d5abf1ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a15e1e-13a0-4ec4-a02b-3c7c95731775",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment = [4.1, 4.8, 7.1, 6.2, 8.3, 5.8, 7.2, 4.3, 8.3, 4.4, 7.7, 7.4]\n",
    "control   = [4.6, 4.6, 3.9, 5.1, 2.8, 4.1, 5.4, 5.8, 4.4, 5.6, 3.8, 4.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed51cbf-7563-42af-a504-98ebf9594695",
   "metadata": {},
   "source": [
    "Here we define the test statistic: $\\mu_{diff} = \\mu_{treatment} - \\mu_{control}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3e72bb-5485-4fae-8369-44efe9a381ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7750000000000004\n"
     ]
    }
   ],
   "source": [
    "mu_treatment = np.mean(treatment)\n",
    "mu_control = np.mean(control)\n",
    "mu_diff = mu_treatment - mu_control\n",
    "print(mu_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bf29a-51ab-460a-943b-7aa03ac4d969",
   "metadata": {},
   "source": [
    "At this point, we can put all the results together (we put them in a DataFrame `df`) and start generating the permutations (shuffling) without \n",
    "replacement for 200 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d7ff474-9be5-4c05-bbf7-5f449974572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'group': ['treatment' if el <= len(treatment) else 'control' for el in range(1,(len(treatment)*2)+1)], \n",
    "                   'responses': treatment + control})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d402e9-8eb0-432e-96d7-5b729bbadf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a953d99-ee17-46be-a235-0f297649c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "mu_diff_samples = []\n",
    "for i in range(n): \n",
    "    # extract a subsample of 50% of data without replacement\n",
    "    df_sample1 = df.sample(frac=0.5, replace=False)\n",
    "    # use set() to get indexes of rows that were not extracted from df in previous step\n",
    "    ix2 = set(df.index) - set(df_sample1.index)\n",
    "    # create dataframe with other 50% of data\n",
    "    df_sample2 = df.iloc[list(ix2)]\n",
    "    # calculate difference in mean from each sample\n",
    "    mu_diff_temp = np.mean(df_sample1['responses']) - np.mean(df_sample2['responses'])\n",
    "    # append sampled mean difference to mu_diff_samples list\n",
    "    mu_diff_samples.append(mu_diff_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b937d-1647-4ca8-91d3-375d12052a63",
   "metadata": {},
   "source": [
    "Now that we've sampled the distribution of the differences in mean, we can plot a histogram or a density curve. Let's also add the observed value `mu_diff` of the difference in means between the treatment and control groups (dashed red line): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1991103a-1b5a-49ea-af76-03ec0cb682cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(mu_diff_samples)\n",
    "plt.axvline(mu_diff, 0, 1, color='r', linestyle='--')\n",
    "plt.show()\n",
    "print('mu_diff:', round(mu_diff, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1deb1-8953-45b7-80d3-27787291102c",
   "metadata": {},
   "source": [
    "It's finally time to compute the **[p-value](https://en.wikipedia.org/wiki/P-value)**. The p-value is an often misunderstood concept, so let's spend a couple of words on it. According to its definition on Wikipedia: \n",
    "\n",
    "> The p-value is the **probability** of obtaining **test results** at least **as extreme as** the result **actually observed**, *under the assumption that the null hypothesis is correct*.\n",
    "\n",
    "Let's unpack this definition: \n",
    "\n",
    "- the first thing to note is that **the p-value is a probability**, therefore it can only assume values in the range $[0, 1]$; \n",
    "- another important piece is the one at the end: *under the assumption that the null hypothesis is correct*; if you recall, the null hypothesis stated that the mean of the two groups was the same and we generated the data in the histogram with this idea (we randomly shuffled the data from the two groups as if they had no difference); \n",
    "- the **test results** are the differences between the sampled means of the two groups that we calculated in the resampling loop (200 iterations); \n",
    "- the result **actually observed** is the difference between the means of the treatment and control groups that we calculated earlier and that we stored in the `mu_diff` object; \n",
    "- finally, the p-value is the probabiliy of observing test results **at least as extreme as** the result actually observed; in simple terms this means that we need to **count how many** test results were >= than the `mu_diff` we observed and divide this number by the total number of test results we computed (200 in our case).\n",
    "\n",
    "Here's a visual explanation of what a p-value is: \n",
    "\n",
    "<img src=\"img/p-value.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710db2e-7c18-4a4c-b688-1a9dae4c2600",
   "metadata": {},
   "source": [
    "Let's count how many times the sampled test results were >= than the actual observed result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686559f2-cdf9-4997-b448-856a5cf8c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(mu_diff_samples>mu_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a498a-12f5-4deb-9e82-dbe1657a160f",
   "metadata": {},
   "source": [
    "Only one observation from our resampling procedure had a result as extreme as the one observed; let's divide it by the total number of samples (200) in order to find the probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d620b81-6e89-406c-b612-cca5135e1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = sum(mu_diff_samples>mu_diff)/n\n",
    "print('p-value: ', p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c441cf3e-6d76-46b5-9350-3e7b71291333",
   "metadata": {},
   "source": [
    "Now, is a p-value of 0.005 (it can also be read in percentage terms as 0.5%) good or bad? Statisticians don't like the idea of leaving to the researcher or analyst the decision of determining whether a result is “too unusual” to happen by chance, therefore **a threshold is specified in advance**, as in “the observed value is more extreme than 5% of the chance results (under $H_0$)”; **this threshold is known as alpha** or as the [significance level](https://en.wikipedia.org/wiki/Statistical_significance). \n",
    "\n",
    "Typical alpha levels are 10%, 5% and 1% (the lower the alpha level, the more stringent the test is), with 5% being the most widely used. \n",
    "\n",
    "> Any chosen level is an arbitrary decision; there is nothing about the process that will guarantee correct decisions x% of the time. This is because the probability question being answered is not \"what is the probability that this happened by chance?\" but rather \"given a chance model (under $H_0$), what is the probability of a result this extreme?\" \n",
    "\n",
    "We choose an alpha level of 5% (that is, $\\alpha = 5\\%$) and thus compare our p-value to this significance level: \n",
    "\n",
    "- if $p-value > \\alpha$: we **accept the null hypothesis** $H_0$ that the result we observed is no different from a chance result; \n",
    "- if $p-value <= \\alpha$: we **reject the null hypothesis** $H_0$ and we say that the observed result (the difference in means between the two groups) is **statistically significant**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3e086-c9c2-465e-94f0-be94daf643c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "if p_value <= alpha: \n",
    "    print('p-value <= alpha | the observed result is statistically significant | H0 rejected')\n",
    "else: \n",
    "    print('p-value > alpha | the observed result is not statistically significant | H0 accepted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc798f12-47f2-461c-9bb2-d11b2f1e5746",
   "metadata": {},
   "source": [
    "In our example, the $p-value=0.005$ is well below the significance level $\\alpha=0.05$, which means that if it's true that the product doesn't have the effect it advertises, then obtaining the observed difference between control and treatment groups by chance, occurs with a probability of only 0.5%. \n",
    "\n",
    "That's a very low probability. In fact, **at our 5% level of significance, we reject the null hypothesis** and accept the alternative: the new product does appear to be working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7c168-870f-48e4-abf0-e0c073f24323",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7431796-2493-4dd5-b659-e94ae723f170",
   "metadata": {},
   "source": [
    "Out of completeness, let's also see how to use a t-test to check for the difference in means between the `treatment` and `control` groups. Again, the null hypothesis is that the means of the two groups are the same and let's choose the sampe significance level of $\\alpha=0.05$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7695bdf0-951a-46ff-bf60-6cf323c48cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(treatment, control, alternative='greater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17df981-a8cb-4a69-90e3-65667e136808",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "if stats.ttest_ind(treatment, control).pvalue <= alpha: \n",
    "    print('p-value <= alpha | the observed result is statistically significant | H0 rejected')\n",
    "else: \n",
    "    print('p-value > alpha | the observed result is not statistically significant | H0 accepted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71ed91-eabd-4233-84cd-a997b4ffe6d2",
   "metadata": {},
   "source": [
    "As you can see, the result is the same, although the test statistic and the p-value are different. Again, the difference between this method and the permutation test is that: \n",
    "- here we are assuming that the underlying distribution follows a Student's t-distribution and thus we compare the test statistic (that has been duly calculated with a [formula](https://en.wikipedia.org/wiki/Student%27s_t-test#Assumptions)) with the theoretical values of the t-distribution; \n",
    "- in a permutation test, we make no assumptions about the data distribution and we compare the test statistic (calculated as the simple difference in means between groups) with the sampled distribution obtaind via the resampling method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d800c7e7-584c-41e9-81bd-4dc081998b26",
   "metadata": {},
   "source": [
    "### AB Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b4cad-8b3a-45e8-a950-cf390b7af375",
   "metadata": {},
   "source": [
    "Although we've briefly mentioned them, AB tests are so frequently used in business, academy and research that it is worth spending a bit more time talking about them. As you will see, we now have all the tools necessary to easily takle them: the design of an experiment and the process of hypothesis testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bcb9ff-3ac5-4b87-90f2-2509ed985f6b",
   "metadata": {},
   "source": [
    "An **[AB test](https://en.wikipedia.org/wiki/A/B_testing)** is an **experiment with two groups** to establish which of two drugs, features, products or the like is better. Often one of the two groups (the control) represents the status-quo. A **typical hypothesis** is that **treatment is better than control**, representing a [one-tail test](https://en.wikipedia.org/wiki/One-_and_two-tailed_tests). \n",
    "\n",
    "AB tests are common in (but not limited to) online businesses and marketing, since the experiment workflow is easily designed and results are readily measured. Below are some examples:\n",
    "- Testing two cart pages to determine which produces more conversions \n",
    "- Testing two paid search ads to determine which generates more clicks\n",
    "- Testing two drugs to determine which suppresses symptoms more effectively \n",
    "- Testing two soil treatments to determine which produces better seed germination "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc0ffe-98db-41ff-b17f-ee3496aa7db4",
   "metadata": {},
   "source": [
    "In an AB test, **subjects** are randomly assigned to groups A and B; the subjects might be web visitors, a group of people, plant seeds and so on; the key idea is that some of the subjects are exposed to the treatment, while others are exposed to the control. \n",
    "\n",
    "The **randomisation process** is key, since this way you know that any difference between groups is due to either:\n",
    "- the effect of the different treatments\n",
    "- the effect of chance, that is the random assignment resulted in the better-performing subjects being concentrated in A or B\n",
    "\n",
    "Particular attention needs to be placed in the **test statistic or metric** used to compare the two groups. One of the most common metrics in a business context is a **binary variable**: \n",
    "- the visitor clicked or not \n",
    "- the customer purchased or not \n",
    "- the transaction resulted in a fraud or not "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad8458-0a0f-4e41-b003-d0e2787d6895",
   "metadata": {},
   "source": [
    "Let's see a practical example. You're a data analyst at an e-commerce and the product team wants to redesign the call-to-action of the website's home page to, hopefully, increase conversion rate. Here are a few important pointers and steps in the design and execution of the experiment: \n",
    "\n",
    "1. the current proportion of people who click on the call-to-action is about 23% of visitors; \n",
    "2. you're hoping to reach at least a 10% absolute increase, reaching a new conversion rate of about 33% with the new redesign; \n",
    "3. using an [online calculator](https://www.evanmiller.org/ab-testing/sample-size.html#!23;80;5;10;0), you find out that you'll need a sample size of at least 288 observations per group (control and test); \n",
    "4. you set-up the test and divert 50% of the website's traffic to the home page with the new call-to-action (test group), while the remaining 50% will continue seeing the usual CTA (control group); \n",
    "5. you let the test run for the [appropriate amount of time](https://www.abtasty.com/blog/how-long-run-ab-test/), according to your iindustry knowledge and business cycle; \n",
    "6. finally, you collect the data and are ready to analyse it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8ce55f-0d31-4764-935b-c29d1d858990",
   "metadata": {},
   "source": [
    "We will perform a **permutation test** to assess wether there is a statistically significant difference between the click-through rates of the two groups; in particular, our **null hypothesis** is that there is no difference between the CTR of the two groups, which we hope to reject in favor of the **one-tail alternative** that the CTR of the test group is higher than the control group: \n",
    "- $H_0: CTR_{test} = CTR_{control}$\n",
    "- $H_1: CTR_{test} > CTR_{control}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f78f186-6ac2-4f4d-8a82-e1797a226b84",
   "metadata": {},
   "source": [
    "You can see our dataset in the `ab` DataFrame, where we have an `id` and `visit_date` columns indicating the single visits, the `group` variable tells us which group that visit was assigned to and finally the `clicked` column indicates if that visitor clicked or not on the call-to-action button. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979fcb9b-6c3b-4ddf-8fc8-d3a18e649cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = pd.read_csv(\"data/ab-test-data.csv\")\n",
    "ab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023274ba-af4b-4f1c-8123-c429180c5d99",
   "metadata": {},
   "source": [
    "There are no missing values and all the data is in the correct format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295cc059-4273-4e17-8b6a-5cc6cc6c7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31af77a-13ad-4902-a502-30c61cc82420",
   "metadata": {},
   "source": [
    "Let's compute a few **descriptive statistics**, where we can see that: \n",
    "- we have a total of 294 observations per group (more than needed to prove a 10% absolute increase); \n",
    "- we can already see that the test group received more clicks than the control group; \n",
    "- notice that the test group reached a CTR well above the 33% mark we set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6f422-1f2c-4c36-acab-c7f26e1e82a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.groupby(['group'], as_index=False).agg({'clicked': ['count', 'sum', np.mean]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698401b6-9f0c-43b5-b4b1-0d1a37c2fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_test = np.mean(ab[ab.group=='test']['clicked'])\n",
    "mu_control = np.mean(ab[ab.group=='control']['clicked'])\n",
    "\n",
    "ab_mu_diff = mu_test - mu_control\n",
    "print('Observed CTR difference:', round(ab_mu_diff, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6d22f-748d-4e96-a320-c099b5a2f9a4",
   "metadata": {},
   "source": [
    "Let's now compute the **permutation test** (the algorithm will be the same as the one we saw in the previous section) and plot the distribution of the resulting sampled statistics (in our case, the differences between the sampled CTRs): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf358c2-fc01-4532-a352-a6a1569a4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "np.random.seed(1)\n",
    "mu_diff_samples = []\n",
    "for i in range(n): \n",
    "    # extract a subsample of 50% of data without replacement\n",
    "    df_sample1 = ab.sample(frac=0.5, replace=False)\n",
    "    # use set() to get indexes of rows that were not extracted from df in previous step\n",
    "    ix2 = set(ab.index) - set(df_sample1.index)\n",
    "    # create dataframe with other 50% of data\n",
    "    df_sample2 = ab.iloc[list(ix2)]\n",
    "    # calculate difference in mean from each sample\n",
    "    mu_diff_temp = np.mean(df_sample1['clicked']) - np.mean(df_sample2['clicked'])\n",
    "    # append sampled mean difference to mu_diff_samples list\n",
    "    mu_diff_samples.append(mu_diff_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289af2c-36b4-41bc-8666-eb88e0d48793",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(mu_diff_samples)\n",
    "plt.axvline(ab_mu_diff, 0, 1, color='r', linestyle='--')\n",
    "plt.show()\n",
    "print('ab_mu_diff:', round(ab_mu_diff, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a55a5c-21e5-4ea3-b5db-27f4a1d6935e",
   "metadata": {},
   "source": [
    "In order to calculate the **p-value**, we compute the proportion of the number of sampled statistics that were as extreme or more extreme than the observed CTR difference between the test and control groups. As you can see from the result, there is only one, therefore the computed p-value is 0.0005. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e637b-c5a4-420f-a4da-c47ade22d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Values more extreme than observed:', sum([el >= ab_mu_diff for el in mu_diff_samples]))\n",
    "p_value = sum([el > ab_mu_diff for el in mu_diff_samples])/n\n",
    "print('p-value:', p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db2d43-e272-44d5-ba89-53621ae92801",
   "metadata": {},
   "source": [
    "This means that, given an **alpha significance level** of 5%, we can safely **reject the null hypothesis** and conclude that the difference in CTR between the test and control groups is **statistically significant**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eaae43-e127-47ca-bf81-2ac0cd4790e8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03524f79-8fa3-4e6e-b2ec-3d44a09f7765",
   "metadata": {},
   "source": [
    "Out of completeness, let's also see how to reach the same results using a z-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b925f-288c-4e05-918a-ac11d3146d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.weightstats import ztest as ztest\n",
    "ab_ztest = ztest(x1=ab[ab.group=='test']['clicked'], \n",
    "                 x2=ab[ab.group=='control']['clicked'], \n",
    "                 alternative='larger')\n",
    "ab_ztest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3686c-9779-43ae-a883-cdf813c1bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('p-value:', ab_ztest[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd1298-728e-4d4b-bc0d-e2c26243be5f",
   "metadata": {},
   "source": [
    "Since the p-value is lower than the $\\alpha = 0.5$ (it's practically very close to zero), we can safely reject the null hypothesis and make the same conclusions as above. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
