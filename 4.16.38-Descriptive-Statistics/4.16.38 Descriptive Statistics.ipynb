{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69760e5-c8c1-479f-a57a-007f45714772",
   "metadata": {},
   "source": [
    "# 4.16.38 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfb515a-1aa4-49ad-a906-4babec10bffb",
   "metadata": {},
   "source": [
    "### A brief history of Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab271a23-386d-4882-8455-65e8105010a9",
   "metadata": {},
   "source": [
    "The etymology of the word **statistics** derives from the Italian word \"stato\" and refers, in almost all European languages, to the observation that the earliest information about real phenomena was collected and organized by the state bodies that were also its main users. \n",
    "\n",
    "- The use of statistical methods dates back to at least the **5th century BCE**, when early empires often collated *censuses of the population* or recorded the *trade in various commodities* and we can date the earliest writings on statistical inference between the **8th and 13th centuries**, back to Arab mathematicians and cryptographers during the Islamic Golden Age. \n",
    "\n",
    "    <figure>\n",
    "    <img src=\"img/altar-domitius-ahenobarb.jpeg\" width=\"900\">\n",
    "    <figcaption align = \"center\"> <i>Stages in a census of the Roman citizen body (base of the altar of Domitius Ahenobarbus)</i></figcaption>\n",
    "    </figure>\n",
    "- The modern mathematical *theory of probability* has its roots in attempts to analyze games of chance by [Gerolamo Cardano](https://en.wikipedia.org/wiki/Gerolamo_Cardano) in the **16th century**, and by [Pierre de Fermat](https://en.wikipedia.org/wiki/Pierre_de_Fermat), [Blaise Pascal](https://en.wikipedia.org/wiki/Blaise_Pascal) and [Jacob Bernoulli](https://en.wikipedia.org/wiki/Jacob_Bernoulli) in the **17th century**.\n",
    "    <img src=\"img/17th-century.png\" width=\"800\">\n",
    "- Thanks to the developments in mathematics and probability theory, the use of statisctics in the modern sense began evolving in the **18th century** in response, among other things, to the novel needs of industrializing sovereign states. There are some particularly noteworthy mentions to be made: [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes) proved *Bayes' theorem*, Laplace and [Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss) discovered the *Normal Distribution* and [William Playfair](https://en.wikipedia.org/wiki/William_Playfair) introduced the idea of *graphical representation* into statistics. \n",
    "        <figure>\n",
    "    <img src=\"img/18th-century.png\" width=\"800\">\n",
    "    <figcaption align = \"center\"> <i>Thomas Bayes (1701-1761), Carl Gauss (1777-1855) and William Playfair (1759-1823)</i></figcaption>\n",
    "    </figure>\n",
    "- The **19th century** saw the discovery of *method of least squares*, which was used to minimize errors in data measurement and is the most used approach to fit data with a *linear regression model*. \n",
    "        <figure>\n",
    "    <img src=\"img/19th-century.png\" width=\"600\">\n",
    "    <figcaption align = \"center\"> <i></i></figcaption>\n",
    "    </figure>\n",
    "- [William Sealy Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset) and [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) were the stars of the progress made in the **20th century**, which involved the development of better *design of experiments* models, *hypothesis testing* and techniques for use with small data samples. \n",
    "        <figure>\n",
    "    <img src=\"img/20th-century.png\" width=\"800\">\n",
    "    <figcaption align = \"center\"> <i>William Sealy Gosset (1876-1937) - Statistician and Brewer</i></figcaption>\n",
    "    </figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc6966-ced0-4b81-8e2e-fc89a93723d6",
   "metadata": {},
   "source": [
    "Check out [this timeline](https://www.statsref.com/timeline.pdf) for a more detailed account of the historical milestones in statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805a56b-d5cf-47ab-9d00-5c605abe8e25",
   "metadata": {},
   "source": [
    "### Statistics today... or Data Science?\n",
    "\n",
    "In **1962**, [John Tukey](https://en.wikipedia.org/wiki/John_Tukey) described a field he called *\"data analysis\"*, which resembles modern data science. In **1985**, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term *\"data science\"* for the first time as an alternative name for statistics.\n",
    "\n",
    "The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In a **2001** paper, he advocated *an expansion of statistics beyond theory into technical areas*; because this would significantly change the field, it warranted a new name.\n",
    "\n",
    "In **2008** [Hal Varian](https://en.wikipedia.org/wiki/Hal_Varian), chief economist at Google, [said](https://www.youtube.com/watch?v=pi472Mi3VLw) that statistics will be *“the sexy profession of the next ten years”*. \n",
    "\n",
    "The professional title of *\"data scientist\"* has been attributed to [DJ Patil](https://en.wikipedia.org/wiki/DJ_Patil) (Chief Data Scientist of the United States Office of Science and Technology Policy from 2015 to 2017) and Jeff Hammerbacher in **2008**.\n",
    "\n",
    "<img src=\"img/21st-century.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c305776-a6bf-488f-bfbf-0171b2c96890",
   "metadata": {},
   "source": [
    "This excursus should clarify the fact that statistics and data science are not the same thing, but they're no strangers either; they are somewhat related: \n",
    "\n",
    "- Statisticians often use the term **estimates** for values calculated from the observed data (such as the mean). This is to differentiate the value we observe from a **sample** (like the estimated mean of a sample of the population) from what the **true theoretical value** actually is (the actual mean of the entire population). \n",
    "- Data scientists and business analysts, on the other end, are more pragmatic and thus they are more likely to refer to such values as a **metric**. \n",
    "\n",
    "This depicts well the **difference in approach between statistics and data science**: the art managing uncertainty lies at the heart of the former, whereas hard business and organizational goals are the focus of the latter. \n",
    "\n",
    "Hence, **statisticians estimate, and data scientists measure**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2a770-e467-4fac-a38c-b999d0f4d45c",
   "metadata": {},
   "source": [
    "### Measures of central tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c75d3-c52d-43cf-949b-6ca256af8ae9",
   "metadata": {},
   "source": [
    "It is often useful to use a summary measure in order to describe a whole set of data with a single value that represents the middle or centre of its **distribution**; such a calculation is called **measure of central tendency**. \n",
    "\n",
    "There are three main measures of central tendency: the **mode**, the **median** and the **mean**. Each of these measures describes a different indication of the typical or central value in the distribution. We will see distributions in more detail later on, but in layman terms *a distribution is a collection of data relative to a variable*. \n",
    "\n",
    "Consider the following DataFrame `df`, containing the height measurements taken from 15 students. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247265ff-3abf-4be6-8df7-ac88da824cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4bb90-c3b9-48c8-a084-621724760aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'id': range(1,16),\n",
    "    'height': [161, 171, 183, 174, 177, 182, 182, 166, 187, 165, 175, 163, 198, 176, 180]})\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061a9b5-b6bb-49eb-a700-aa3ce9294364",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(x)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa35f1-010f-4aa7-9c7e-e32b7d32e630",
   "metadata": {},
   "source": [
    "#### Mode\n",
    "\n",
    "The mode is the most frequent value in a distribution. Therefore, to find out what the mode of a distribution is, you simply need to **find the most frequent value** in your variable of interest. \n",
    "\n",
    "To do this, let's group the `height`s together, count the occurrences for each height value and finally sort the resulting **frequency table** from highest to lowest count. The first value will then be the mode of the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14871df9-d7b4-40c6-8104-64f3050d882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('height', as_index=False).size().sort_values('size', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed141cd1-ce12-4404-bfb1-8a0d0e84098f",
   "metadata": {},
   "source": [
    "The `stats` module in the `scipy` library contains a `mode()` function that can be used to return the mode of a list or series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bea63-3996-43dc-9c74-1e8720197c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.mode(df.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d787648-863d-45ec-acdf-2ff5d89277fa",
   "metadata": {},
   "source": [
    "Let's re-create a function that, given a DataFrame column as input, returns its mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25262c06-f5ce-40a9-836a-a92e47d81603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33992f-ed3a-4948-b84c-80ec4318773d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_mode(df, col): \n",
    "    freq_tbl = df.groupby(col, as_index=False).size().sort_values('size', ascending=False)\n",
    "    return freq_tbl.head(1)[col].values[0]\n",
    "\n",
    "my_mode(df, 'height')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9c6af-31d6-4ad4-ad83-358d1b91db48",
   "metadata": {},
   "source": [
    "#### Arithmetic Mean\n",
    "\n",
    "Commonly known as the average, the arithmetic mean is given by the sum of the value of each observation in a dataset divided by the number of observations. Given a variable $x$ containing $n$ elements, the **formula to calculate the mean** is the following: \n",
    "\n",
    "$$\n",
    "    \\frac{1}{n} \\sum \\limits_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "Where the big $\\sum$ symbol means that we **sum** all the elements in $x$ from $1$ to $n$. \n",
    "\n",
    "*Note: the $i$ has a role similar to the one of the counter in a for loop: at each cycle it keeps track of the elements of the variable $x$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3475c469-f1e9-4202-972d-5a29cae5a04a",
   "metadata": {},
   "source": [
    "This is how we would calculate the arithmentic mean of the `height` column in Python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f50aed-f5fc-49e8-96c5-277444d768a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df.height)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9fc783-8d46-4792-993a-71a4c8092acd",
   "metadata": {},
   "source": [
    "The `numpy` library also has a handy function `np.mean()` that allows you to do just that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e8ac3-39b0-4ea1-9fe1-64135d7a8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30215019-a145-4f06-afe1-923d78e482cc",
   "metadata": {},
   "source": [
    "Let's re-create a function that, given a DataFrame column as input, returns its arithmetic mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8df4b-aea8-461c-a25c-257c5c2c3c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82471b6-510b-46e2-81c9-a77013bfc55e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_mean(df, col): \n",
    "    n = len(df[col])\n",
    "    s = 0\n",
    "    for i in range(n): \n",
    "        s = s + df[col][i]\n",
    "    return s/n\n",
    "\n",
    "my_mean(df, 'height')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d65cca2-2823-4b8a-b868-1a8db73f9ac3",
   "metadata": {},
   "source": [
    "#### Median\n",
    "\n",
    "Although the arithmetic mean is a useful and handy measure, it is **not a robust statistic** and thus may not always coincide with one's notion of \"middle\". This is especially true when there are extreme values or outliers; the arithmetic mean, in fact, is very sensitive to the exact values of all the variables in the series, and thus can be strongly influenced by extreme values. \n",
    "\n",
    "The **median**, on the other hand, is the middle value of a data series. In other words, given an ordered (ascending) list or series of data, it is the value that separates the first 50% from the last 50% of data points. \n",
    "\n",
    "So, if you want to find the median of a data series, you first need to order the data from lower to higher and then you need to find the position of the element that separates the higher half from the lower half of your data. The formula varies depending on whether the number of elements in your series is odd or even: \n",
    "\n",
    "$$\n",
    "    \\begin{equation}\n",
    "      \\begin{cases}\n",
    "        x[\\frac{n-1}{2}] & \\text{if $n$ is odd}\\\\\n",
    "        \\frac{x[\\frac{n-1}{2}] + x[\\frac{n+1}{2}]}{2} & \\text{if $n$ is even}\n",
    "      \\end{cases}\n",
    "    \\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ff8d4-1177-4ffd-a808-904627fa3fa3",
   "metadata": {},
   "source": [
    "Although the formula may look complicated, the procedure is much easier than what it may seem: \n",
    "\n",
    "- if you have an **odd number of values**, you take the one in the middle (after sorting): <br />\n",
    "    $[1,\\text{ } 4,\\text{ } \\boldsymbol{6},\\text{ } 8,\\text{ } 9]$ <br />\n",
    "    $median = 6$\n",
    "- if you have an **even number of values**, you take the average of the two middle elements (after sorting): <br />\n",
    "    $[1,\\text{ } 4,\\text{ } \\boldsymbol{6},\\text{ } \\boldsymbol{8},\\text{ } 9,\\text{ } 9]$ <br />\n",
    "    $median = (6+8)\\text{ }/\\text{ }2 = 7$\n",
    "\n",
    "Let's see an example using the `df` DataFrame from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cca060-52ca-47e0-b9a5-955b0566d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we sort the DataFrame by our variable of interest in ascending order\n",
    "df = df.sort_values('height')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604f738-27af-48a3-8688-d0b283943d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we check if the number of elements is odd or even\n",
    "if n%2==0: \n",
    "    print(n, 'is even')\n",
    "else: \n",
    "    print(n, 'is odd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e09da-0f81-4d0b-ac10-26882958bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we apply the corrensponding formula to get the median value\n",
    "df.height.iloc[int((n-1)/2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc69689-be2e-4046-af32-3cc8b4a963f4",
   "metadata": {},
   "source": [
    "Again, the numpy library also has a handy function np.median() that we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448f33d-7d77-4860-8fe0-c2a0acfedabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(df.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db68150-1598-4056-971b-312ced99433c",
   "metadata": {},
   "source": [
    "### Measures of statistical dispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af33a27-681b-4bae-9ccd-996f3765eb78",
   "metadata": {},
   "source": [
    "Call it dispersion, variability or spread, sometimes you may need to know how similar or varied the set of data you're studying is. That is, whether your data points are **tightly clustered** (low dispersion) or whether they are **spread out** (high dispersion). \n",
    "\n",
    "> Variability lies at the heart of statistics: measuring it, reducing it, distinguishing random from real variability, identifying the various sources of real variability, and making decisions in the presence of it. \n",
    "\n",
    "Look at the two figures below, imagine that those data points are people standing in line: it is clear that those in the right figure are much more **spread** apart than those in the left one. So we will say that **the data series on the right has a higher dispersion** than the one on the left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238fc1b-bd95-4207-8502-af8ffbb2281e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# generate sample data\n",
    "points = [n for n in range(1,21)]\n",
    "series = ['', '2', '', '', '5', '', '', '', '9', '', '', '', '13', '', '15', '', '', '', '19', '']\n",
    "series2 = ['', '', '', '', '', '', '', '8', '9', '10', '11', '12', '13', '', '', '', '', '', '', '']\n",
    "\n",
    "# Create figure and plot a stem plot with the date\n",
    "fig, ax = plt.subplots(1, 2) #figsize=(8.8, 2), constrained_layout=True)\n",
    "fig.set_size_inches(18.5, 2)\n",
    "\n",
    "#######################################\n",
    "### first chart: low dispersion series\n",
    "ax[0].set(title=\"Low dispersion data series\")\n",
    "\n",
    "#ax.vlines(dates, 0, 5, color=\"tab:red\")  # The vertical stems.\n",
    "ax[0].plot(points, np.zeros_like(points), \"-o\",\n",
    "        color=\"k\", markerfacecolor=\"w\")  # Baseline and markers on it.\n",
    "\n",
    "# annotate lines\n",
    "for d, r in zip(points, series2):\n",
    "    ax[0].annotate(r, xy=(d, 0.01), horizontalalignment=\"center\")\n",
    "\n",
    "# remove y axis and spines\n",
    "ax[0].yaxis.set_visible(False)\n",
    "ax[0].xaxis.set_visible(False)\n",
    "ax[0].spines[[\"left\", \"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "ax[0].margins(y=0.1)\n",
    "\n",
    "#######################################\n",
    "### second chart: high dispersion series\n",
    "ax[1].set(title=\"High dispersion data series\")\n",
    "\n",
    "#ax.vlines(dates, 0, 5, color=\"tab:red\")  # The vertical stems.\n",
    "ax[1].plot(points, np.zeros_like(points), \"-o\",\n",
    "        color=\"k\", markerfacecolor=\"w\")  # Baseline and markers on it.\n",
    "\n",
    "# annotate lines\n",
    "for d, r in zip(points, series):\n",
    "    ax[1].annotate(r, xy=(d, 0.01), horizontalalignment=\"center\")\n",
    "\n",
    "# remove y axis and spines\n",
    "ax[1].yaxis.set_visible(False)\n",
    "ax[1].xaxis.set_visible(False)\n",
    "ax[1].spines[[\"left\", \"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "ax[1].margins(y=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab39cf-62ca-4c9e-8a98-7c3ee3c5a622",
   "metadata": {},
   "source": [
    "So why should we care about the dispersion of our data? Generally, summarising the dataset is key to understanding its composition, especially when the dataset is large. \n",
    "\n",
    ">As we've seen in the previous section, the mode, mean and median summarise the data into a single value that is typical or representative of all the values in the dataset. Measures of spread summarise the data in a way that shows how scattered the values are and how much they differ from the mean value.\n",
    "\n",
    "For example, if you calculate the mean of the two datasets above, you'll find that they are the same (10.5), but a quick look at their graphical representation should convince you that they have a different composition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e73359-eabc-4908-ba6a-2e1311d8cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "lo = [8,9,10,11,12,13]\n",
    "hi = [2,5,9,13,15,19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba9fb5-9444-43e4-86f4-cb2238b8f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean of low dispersion data series:', np.mean(lo))\n",
    "print('Mean of high dispersion data series:', np.mean(hi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7cb8b4-a9ce-48d7-a2ec-f7d0a3a48634",
   "metadata": {},
   "source": [
    "#### Range\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67028161-4101-4166-980f-8a54eb3d11b9",
   "metadata": {},
   "source": [
    "The simplest measure of dispersion is the **range**, which tells us how far apart are the opposite extreme points in the dataset. It is calculated as the **difference between the highest and lowest values** in a list or data series: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c655696-41d5-4177-a3b0-4ba421f2e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_lo = max(lo) - min(lo)\n",
    "range_hi = max(hi) - min(hi)\n",
    "\n",
    "print('Range of low dispersion data series:', range_lo)\n",
    "print('Range of high dispersion data series:', range_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da8706-a65e-402b-8c6f-46834c9f0bbb",
   "metadata": {},
   "source": [
    "Of course, as you can imagine, this measure is also quite susceptible to the presence of extreme values or outliers and should be used with caution. A more robust alternative is the interquartile range (IQR), which we'll explore next. \n",
    "\n",
    "*As an important side note, keep in mind that **extreme values and outliers are not the same thing**; check out the picture below for a quick reference or make sure to watch [this video](§https://www.youtube.com/watch?v=CktqqnZf6LI) for a more technical dissertation around Extreme Value Theory.*\n",
    "\n",
    "<img src=\"img/extremes-outliers.jpeg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90514a1f-bc50-4d90-b6a1-edd21f00d3a0",
   "metadata": {},
   "source": [
    "#### Interquartile range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180915a3-7324-4f0a-ac25-9dd1945d15d6",
   "metadata": {},
   "source": [
    "Before we can talk about **interquartile range (IQR)**, we should first mention what a **quartile** is. Just like the median is the value that divides a set of data between the lowest and highest halves, quartiles divide an ordered dataset into four equal parts. Specifically, they refer to the values of the point between the quarters. \n",
    "\n",
    "<img src=\"img/quartiles.png\" width=\"900\">\n",
    "\n",
    "*Note: a dataset may also be divided into quintiles (five equal parts) or deciles (ten equal parts) or percentiles (one hundred equal parts).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8766c-da72-46e4-8763-f935ebc31b6e",
   "metadata": {},
   "source": [
    "In Python we can use numpy's `np.percentile()` function to divide our data in as many equal parts as we like. In order to calculate the quartiles, and thus divide our data in four equal parts (Q1, Q2 and Q3), we proceed as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc52fd-523e-4966-92fd-7f5fb03b752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(lo, [25, 50, 75]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbeeb19-d80f-40c0-8feb-8f10136c3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(hi, [25, 50, 75]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c926d-eb23-4fe2-8d6a-7e3dfb7603db",
   "metadata": {},
   "source": [
    "For the low dispersion dataset: \n",
    "\n",
    "- Q1: the first quartile is 9.25\n",
    "- Q2: the second quartile (the median) is 10.5\n",
    "- Q3: the third quartile is 11.75\n",
    "\n",
    "For the high dispersion dataset: \n",
    "\n",
    "- Q1: the first quartile is 6.0\n",
    "- Q2: the second quartile (the median) is 11.0\n",
    "- Q3: the third quartile is 14.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8deb74-546f-44b0-8ba9-4be1de1ed69e",
   "metadata": {},
   "source": [
    "At this point we can introduce the concept of the **interquartile range (IQR)**, which is the difference between the upper (Q3) and lower (Q1) quartiles, and **describes the middle 50% of values** when ordered from lowest to highest. As we've seen before, the IQR is often seen as a better measure of dispersion than the range as it is not affected by extreme values or outliers.\n",
    "\n",
    "Let's calculate the IQR for our two datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a6bf6-e32f-43af-8795-6da5a06db97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_lo, q3_lo = np.percentile(lo, [25, 75])\n",
    "q1_lo, q3_lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cfd44b-e36b-45a2-99f8-cd211ce81d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_hi, q3_hi = np.percentile(hi, [25, 75])\n",
    "q1_hi, q3_hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336bdcc-a41a-46c0-ae30-a2096312f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_range_lo = q3_lo - q1_lo\n",
    "iqr_range_hi = q3_hi - q1_hi\n",
    "\n",
    "print('Range of low dispersion data series:', iqr_range_lo)\n",
    "print('Range of high dispersion data series:', iqr_range_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8b575-b889-47c8-98f9-3969f168faaa",
   "metadata": {},
   "source": [
    "#### Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265a724-359c-4920-9f92-b636a5d9327e",
   "metadata": {},
   "source": [
    "The variance and the standard deviation are **measures of dispersion of the data around the mean**. They summarise how close each observed data value is to the mean value.\n",
    "\n",
    "Variance ($\\sigma^2$) and standard deviation ($\\sigma$) are directly related since the latter is the square root of the former. So, in order to calculate the standard deviation, we first need to compute the variance of our data. \n",
    "\n",
    "Let's take our high dispersion dataset from before as an example. The variance of a series of data points is computed by looking at how far away each data point is from the mean value. The distances from each data point ($x$) and the overall mean ($\\mu$) are referred to as **deviations from the mean**. Check out the figure below for a visual explanation: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b73499-3793-4a91-a059-25cdf8419fd3",
   "metadata": {},
   "source": [
    "<img src=\"img/variance-compute.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34638242-40a1-425a-9626-393105e3c079",
   "metadata": {},
   "source": [
    "To **compute the variance** of these data point, we need to: \n",
    "\n",
    "1. calculate all the deviations from the mean $(x-\\mu)$ in the data set \n",
    "2. square them \n",
    "3. take their average \n",
    "\n",
    "Now, although the last point may seem quite harmless, it hides an important point that needs to be cleared out. At the beginning of this document, we made a distinction between statisticians and data scientists, namely that the former estimate and the latter measure. As a direct consequence of this nuance, for a statistician there are two important cases to consider when calculating a measure such as the variance or the standard deviation (or the mean, for that matter): \n",
    "\n",
    "- is the data we're observing representative of the entire **population**? \n",
    "- or is it the result of a **sample** of a population? \n",
    "\n",
    "Imagine that you are interested in studying the income variability in Italy and you take the two following approaches: \n",
    "\n",
    "1. you take a census of the italian population and ask every single person their income \n",
    "2. you identify a representative subset of the population and ask them their income\n",
    "\n",
    "In the former case, you're studying your phenomena of interest with respect to its entire population, whereas in the latter case, your analysis will be limited to a sample of the actual population. \n",
    "\n",
    "It won't come as a surprise the fact that there are two ways to compute a statistic such as the variance: one that references to data coming from a **population** ($\\sigma^2$) and one for data coming from a **sample** ($s^2$). \n",
    "\n",
    "Let's see the two formulas:\n",
    "\n",
    "$$\n",
    "    \\sigma^2 = \\frac{1}{N} \\sum \\limits_{i=1}^{N} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "    s^2 = \\frac{1}{n-1} \\sum \\limits_{i=1}^{n} (x_i - \\overline{x})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c02a20-9a99-4ab2-a745-8848ff7a03aa",
   "metadata": {},
   "source": [
    "The first main difference is notational: \n",
    "\n",
    "- $\\sigma^2$, $N$ and $\\mu$ for the population \n",
    "- $s^2$, $n$ and $\\overline{x}$ for the sample \n",
    "\n",
    "The second (and more important) difference lies in the denominator of the average: \n",
    "\n",
    "- in the population formula we divide by the **total number of observations** ($N$)\n",
    "- in the population formula we divide by the **total number of observations minus 1** ($n-1$)\n",
    "\n",
    "This difference reflects the fact that in the latter case, we do not have information on the whole population, therefore we are more prone to make a mistake and thus we try to **account for this potential error** by making our estimate a little bit larger than what it actually is (but still close enough) by dividing by $n-1$ \n",
    "\n",
    "> *Note: this isn't all, the [full story](https://en.wikipedia.org/wiki/Bessel%27s_correction) involves degrees of freedom and unbiased estimators, but we don't need to worry about those for now.* \n",
    "\n",
    "Again, statistitians estimate, data scientists measure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4787f6a6-fe69-4286-9340-9b7b0bfcdd14",
   "metadata": {},
   "source": [
    "The **standard deviation** is much easier to interpret than the variance since it is on the same scale as the original data and, since the standard deviation is the square root of the variance, it should be easy enough to derive its formula: \n",
    "\n",
    "$$\n",
    "    \\sigma = \\sqrt{ \\frac{1}{N} \\sum \\limits_{i=1}^{N} (x_i - \\mu)^2 }\n",
    "$$\n",
    "\n",
    "$$\n",
    "    s = \\sqrt{ \\frac{1}{n-1} \\sum \\limits_{i=1}^{n} (x_i - \\overline{x})^2 }\n",
    "$$\n",
    "\n",
    "So, to **compute the standard deviation** of a series of data points, we need to: \n",
    "\n",
    "1. calculate all the deviations from the mean $(x-\\mu)$ in the data set \n",
    "2. square them \n",
    "3. take their average (choosing between $n$ and $n-1$ as needed)\n",
    "4. take the squared root of the result\n",
    "\n",
    "Let's compute the standard deviation for both the high and low dispersion data series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af795a8-f61c-485d-b3f8-2e1a0d556730",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('High dispersion data:', hi)\n",
    "hi_m = np.mean(hi)\n",
    "print('High dispersion mean:', hi_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f0c2a-db0e-463b-9181-9d9f57d2572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. calculate all the deviations from the mean (𝑥−𝜇) in the data set\n",
    "dev_hi = [x-hi_m for x in hi]\n",
    "dev_hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310338df-65bf-470d-b6b6-ce15542c4a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. square the deviations from the mean\n",
    "sq_dev_hi = [el**2 for el in dev_hi]\n",
    "sq_dev_hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c587bf84-2224-41d0-9ea2-8749a5337e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. take their average (let's use the sampling version: 𝑛−1)\n",
    "avg_sq_dev_hi = sum(sq_dev_hi)/(len(hi)-1)\n",
    "avg_sq_dev_hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919db026-54a1-479f-bbb0-b686f2860ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. take the squared root of the result\n",
    "stdev_hi = np.sqrt(avg_sq_dev_hi)\n",
    "stdev_hi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3178b672-1e37-4c98-94ca-7e9fc26a01ca",
   "metadata": {},
   "source": [
    "`numpy` has handy functions to calculate both the variance `np.var()` and the standard deviation `np.std()` of a series of values; just remember to set to 1 the parameter `ddof=1` (delta degrees of freedom) if you're interested in the sample statistic as opposed to the population one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25246112-1489-4629-af62-6ca39f25e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(hi, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f719ca-35be-441c-b0fd-9ebfb8221afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(hi, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c04d3d-20f0-47ae-a1f5-8100dfce7d8e",
   "metadata": {},
   "source": [
    "### Final Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f71cef-fd35-47f8-ad81-52cd5838b7b4",
   "metadata": {},
   "source": [
    "**Measures of Central Tendency**\n",
    "\n",
    "A basic step in exploring and understanding your data is getting a *typical value* for each feature or variable of your dataset: an estimate of where most of the data is located, that is, its central tendency. Check out the summary table below and for further reading check out this [online book](https://openstax.org/books/introductory-statistics/pages/2-5-measures-of-the-center-of-the-data). \n",
    "\n",
    "|  | Mode | Mean | Median | \n",
    "| --- | :---: | :---: | :---: |\n",
    "| **Definition** | Most common value </br > in the data | Sum of values divided </br > by count of values | Middle value in </br > the ordered data |\n",
    "| **Is robust?** | Yes | No | Yes | \n",
    "| **Function** | `stats.mode()` | `np.mean()` | `np.median()` | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41898b60-ccef-41e8-a706-fce3d7e25b1c",
   "metadata": {},
   "source": [
    "**Measures of statistical dispersion**\n",
    "\n",
    "Location is just one dimension in summarizing a feature. A second dimension, dispersion, measures whether the data values are relatively close to each other or spread out. Check out the summary table below and for further reading check out this [online book](https://openstax.org/books/introductory-statistics/pages/2-7-measures-of-the-spread-of-the-data).\n",
    "\n",
    "|  | Range | Interquartile Range | Standard Deviation | \n",
    "| --- | :---: | :---: | :---: |\n",
    "| **Definition** | Max. minus Min. </br > values in the data | Central 50% of </br > the ordered data | Squared root of the average </br > squared deviations from the mean |\n",
    "| **Is robust?** | No | Yes | No | \n",
    "| **Function** | `max() - min()` | `stats.iqr()` | `np.std()` | "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
